{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b6883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             classification_report, roc_curve)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e000aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Loading Data\n",
      "============================================================\n",
      "Training data shape: (7963, 26)\n",
      "Test data shape: (800, 25)\n",
      "\n",
      "First few rows of training data:\n",
      "  patient_id  age     sex  chol       bp  hr  diabetes  family_history  \\\n",
      "0    BMW7812   67    Male   208   158/88  72         0               0   \n",
      "1    CZE1114   21    Male   389   165/93  98         1               1   \n",
      "2    BNI9906   21  Female   324   174/99  72         1               0   \n",
      "3    JLN3497   84    Male   383  163/100  73         1               1   \n",
      "4    GFO8847   66    Male   318    91/88  93         1               1   \n",
      "\n",
      "   smoking  obesity  ...  sedentary_hr  income        bmi  triglycerides  \\\n",
      "0        1        0  ...      6.615001  261404  31.251233            286   \n",
      "1        1        1  ...      4.963459  285768  27.194973            235   \n",
      "2        0        0  ...      9.463426  235282  28.176571            587   \n",
      "3        1        0  ...      7.648981  125640  36.464704            378   \n",
      "4        1        1  ...      1.514821  160555  21.809144            231   \n",
      "\n",
      "   phys_act_days  sleep_hr    country      continent           hemisphere  \\\n",
      "0              0         6  Argentina  South America  Southern Hemisphere   \n",
      "1              1         7     Canada  North America  Northern Hemisphere   \n",
      "2              4         4     France         Europe  Northern Hemisphere   \n",
      "3              3         4     Canada  North America  Northern Hemisphere   \n",
      "4              1         5   Thailand           Asia  Northern Hemisphere   \n",
      "\n",
      "   heart_attack_risk  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Loading Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load training and test datasets\n",
    "# Replace these paths with your actual file paths\n",
    "train_df = pd.read_csv(r'D:\\5th sem\\EM06_Sindorai\\AI\\Heart_Attack_training_dataset.csv')\n",
    "test_df = pd.read_csv(r'D:\\5th sem\\EM06_Sindorai\\AI\\Hear_Attack_evaluation_dataset.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb5a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\user\\.conda\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\user\\.conda\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\user\\.conda\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (1.7.1)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\user\\.conda\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\user\\.conda\\envs\\myenv\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Downloading imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a196b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: Exploratory Data Analysis\n",
      "============================================================\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7963 entries, 0 to 7962\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   patient_id         7963 non-null   object \n",
      " 1   age                7963 non-null   int64  \n",
      " 2   sex                7963 non-null   object \n",
      " 3   chol               7963 non-null   int64  \n",
      " 4   bp                 7963 non-null   object \n",
      " 5   hr                 7963 non-null   int64  \n",
      " 6   diabetes           7963 non-null   int64  \n",
      " 7   family_history     7963 non-null   int64  \n",
      " 8   smoking            7963 non-null   int64  \n",
      " 9   obesity            7963 non-null   int64  \n",
      " 10  alcohol            7963 non-null   int64  \n",
      " 11  exercise_hr_wk     7963 non-null   float64\n",
      " 12  diet               7963 non-null   object \n",
      " 13  prev_heart_prob    7963 non-null   int64  \n",
      " 14  med_use            7963 non-null   int64  \n",
      " 15  stress_lvl         7963 non-null   int64  \n",
      " 16  sedentary_hr       7963 non-null   float64\n",
      " 17  income             7963 non-null   int64  \n",
      " 18  bmi                7963 non-null   float64\n",
      " 19  triglycerides      7963 non-null   int64  \n",
      " 20  phys_act_days      7963 non-null   int64  \n",
      " 21  sleep_hr           7963 non-null   int64  \n",
      " 22  country            7963 non-null   object \n",
      " 23  continent          7963 non-null   object \n",
      " 24  hemisphere         7963 non-null   object \n",
      " 25  heart_attack_risk  7963 non-null   int64  \n",
      "dtypes: float64(3), int64(16), object(7)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "patient_id           0\n",
      "age                  0\n",
      "sex                  0\n",
      "chol                 0\n",
      "bp                   0\n",
      "hr                   0\n",
      "diabetes             0\n",
      "family_history       0\n",
      "smoking              0\n",
      "obesity              0\n",
      "alcohol              0\n",
      "exercise_hr_wk       0\n",
      "diet                 0\n",
      "prev_heart_prob      0\n",
      "med_use              0\n",
      "stress_lvl           0\n",
      "sedentary_hr         0\n",
      "income               0\n",
      "bmi                  0\n",
      "triglycerides        0\n",
      "phys_act_days        0\n",
      "sleep_hr             0\n",
      "country              0\n",
      "continent            0\n",
      "hemisphere           0\n",
      "heart_attack_risk    0\n",
      "dtype: int64\n",
      "\n",
      "Basic Statistics:\n",
      "               age         chol           hr     diabetes  family_history  \\\n",
      "count  7963.000000  7963.000000  7963.000000  7963.000000     7963.000000   \n",
      "mean     53.685922   259.574658    75.102474     0.653020        0.489640   \n",
      "std      21.237794    80.824235    20.583831     0.476039        0.499924   \n",
      "min      18.000000   120.000000    40.000000     0.000000        0.000000   \n",
      "25%      35.000000   191.000000    57.000000     0.000000        0.000000   \n",
      "50%      54.000000   259.000000    75.000000     1.000000        0.000000   \n",
      "75%      72.000000   329.000000    93.000000     1.000000        1.000000   \n",
      "max      90.000000   400.000000   110.000000     1.000000        1.000000   \n",
      "\n",
      "           smoking      obesity      alcohol  exercise_hr_wk  prev_heart_prob  \\\n",
      "count  7963.000000  7963.000000  7963.000000     7963.000000      7963.000000   \n",
      "mean      0.896521     0.504207     0.597890        9.999996         0.495165   \n",
      "std       0.304602     0.500014     0.490355        5.778830         0.500008   \n",
      "min       0.000000     0.000000     0.000000        0.002442         0.000000   \n",
      "25%       1.000000     0.000000     0.000000        4.979003         0.000000   \n",
      "50%       1.000000     1.000000     1.000000       10.062622         0.000000   \n",
      "75%       1.000000     1.000000     1.000000       15.038864         1.000000   \n",
      "max       1.000000     1.000000     1.000000       19.998709         1.000000   \n",
      "\n",
      "           med_use   stress_lvl  sedentary_hr         income          bmi  \\\n",
      "count  7963.000000  7963.000000   7963.000000    7963.000000  7963.000000   \n",
      "mean      0.498179     5.472812      5.976430  158283.114279    28.891190   \n",
      "std       0.500028     2.858878      3.465806   80715.248604     6.318153   \n",
      "min       0.000000     1.000000      0.001263   20062.000000    18.002337   \n",
      "25%       0.000000     3.000000      2.978895   87835.000000    23.420212   \n",
      "50%       0.000000     5.000000      5.904138  157830.000000    28.779074   \n",
      "75%       1.000000     8.000000      8.995663  228486.500000    34.320578   \n",
      "max       1.000000    10.000000     11.999313  299954.000000    39.997211   \n",
      "\n",
      "       triglycerides  phys_act_days     sleep_hr  heart_attack_risk  \n",
      "count    7963.000000    7963.000000  7963.000000        7963.000000  \n",
      "mean      418.370715       3.486877     7.022353           0.343966  \n",
      "std       223.794036       2.278560     1.987275           0.475060  \n",
      "min        30.000000       0.000000     4.000000           0.000000  \n",
      "25%       227.500000       2.000000     5.000000           0.000000  \n",
      "50%       419.000000       3.000000     7.000000           0.000000  \n",
      "75%       614.000000       5.000000     9.000000           1.000000  \n",
      "max       800.000000       7.000000    10.000000           1.000000  \n",
      "\n",
      "Target Variable Distribution:\n",
      "heart_attack_risk\n",
      "0    5224\n",
      "1    2739\n",
      "Name: count, dtype: int64\n",
      "Class balance: heart_attack_risk\n",
      "0    0.656034\n",
      "1    0.343966\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Exploratory Data Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData Info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(train_df.describe())\n",
    "\n",
    "# Check target variable distribution\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(train_df['heart_attack_risk'].value_counts())\n",
    "print(f\"Class balance: {train_df['heart_attack_risk'].value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425c88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Robust preprocessing:\n",
    "      - Safe BP parse (coerce bad/missing)\n",
    "      - Impute numeric/categorical\n",
    "      - Encode categorical (with safe fallback)\n",
    "      - Frequency-encode geo fields\n",
    "      - Ensure risk columns exist and are numeric/binary\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ---- Safe BP parsing ----\n",
    "    # Accepts \"120/80\", \" 120 / 80 \", floats-as-strings; coerces bad to NaN\n",
    "    if 'bp' in df.columns:\n",
    "        bp_parsed = df['bp'].astype(str).str.extract(\n",
    "            r'^\\s*(?P<systolic>\\d+\\.?\\d*)\\s*/\\s*(?P<diastolic>\\d+\\.?\\d*)\\s*$'\n",
    "        )\n",
    "        df['systolic_bp'] = pd.to_numeric(bp_parsed['systolic'], errors='coerce')\n",
    "        df['diastolic_bp'] = pd.to_numeric(bp_parsed['diastolic'], errors='coerce')\n",
    "        df = df.drop(columns=['bp'], errors='ignore')\n",
    "\n",
    "    # ---- Ensure risk columns exist (fill missing with 0) ----\n",
    "    risk_cols = ['diabetes', 'smoking', 'obesity', 'family_history',\n",
    "                 'prev_heart_prob', 'alcohol']\n",
    "    for c in risk_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "        # coerce to numeric (e.g., \"Yes\"/\"No\" or True/False)\n",
    "        df[c] = pd.to_numeric(df[c].map({'Yes':1, 'No':0, True:1, False:0}).fillna(df[c]), errors='coerce')\n",
    "\n",
    "    # ---- Basic imputations by type ----\n",
    "    # Identify numerical columns (avoid target in train)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if is_train and 'heart_attack_risk' in num_cols:\n",
    "        num_cols.remove('heart_attack_risk')\n",
    "\n",
    "    # Categorical columns\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Categorical fill (mode; handle all-NaN with 'Unknown')\n",
    "    for c in cat_cols:\n",
    "        mode_val = df[c].mode(dropna=True)\n",
    "        fill_val = mode_val.iloc[0] if not mode_val.empty else 'Unknown'\n",
    "        df[c] = df[c].fillna(fill_val)\n",
    "\n",
    "    # Binary encode 'sex'\n",
    "    if 'sex' in df.columns:\n",
    "        df['sex'] = df['sex'].map({'Male': 1, 'Female': 0})\n",
    "        # If unexpected values existed, they are NaN now; fill with mode (default 0)\n",
    "        df['sex'] = df['sex'].fillna(df['sex'].mode(dropna=True).iloc[0] if not df['sex'].mode(dropna=True).empty else 0)\n",
    "\n",
    "    # Ordinal encode 'diet'\n",
    "    if 'diet' in df.columns:\n",
    "        diet_mapping = {'Poor': 0, 'Average': 1, 'Healthy': 2}\n",
    "        df['diet'] = df['diet'].map(diet_mapping)\n",
    "        df['diet'] = pd.to_numeric(df['diet'], errors='coerce')\n",
    "\n",
    "    # Geo freq-encoding (train/test safe)\n",
    "    for col in ['country', 'continent', 'hemisphere']:\n",
    "        if col in df.columns:\n",
    "            freq = df[col].value_counts(normalize=True)\n",
    "            df[col + '_freq'] = df[col].map(freq)\n",
    "            df = df.drop(columns=[col], errors='ignore')\n",
    "\n",
    "    # Numeric impute (median)\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9b4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "train_processed = preprocess_data(train_df, is_train=True)\n",
    "test_processed = preprocess_data(test_df, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b4463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed training data shape: (7963, 27)\n",
      "Processed test data shape: (800, 26)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProcessed training data shape: {train_processed.shape}\")\n",
    "print(f\"Processed test data shape: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56acd4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: Feature Engineering\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 4: FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Feature Engineering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create domain features and ensure numeric dtypes.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Guard rails: if these exist but still have NaNs, fill before ops\n",
    "    for col in ['systolic_bp', 'diastolic_bp', 'exercise_hr_wk', 'phys_act_days',\n",
    "                'sleep_hr', 'sedentary_hr', 'stress_lvl', 'age', 'bmi']:\n",
    "        if col in df.columns and df[col].isna().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # Pulse pressure\n",
    "    if {'systolic_bp', 'diastolic_bp'}.issubset(df.columns):\n",
    "        df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    else:\n",
    "        df['pulse_pressure'] = 0.0\n",
    "\n",
    "    # Composite health score\n",
    "    for needed in ['exercise_hr_wk','phys_act_days','sleep_hr','sedentary_hr','stress_lvl']:\n",
    "        if needed not in df.columns:\n",
    "            df[needed] = 0.0\n",
    "    df['health_score'] = (\n",
    "        df['exercise_hr_wk'] +\n",
    "        df['phys_act_days'] +\n",
    "        df['sleep_hr'] -\n",
    "        df['sedentary_hr'] -\n",
    "        df['stress_lvl']\n",
    "    )\n",
    "\n",
    "    # Risk factor count (coerce common yes/no/boolean to 0/1)\n",
    "    risk_cols = ['diabetes', 'smoking', 'obesity', 'family_history',\n",
    "                 'prev_heart_prob', 'alcohol']\n",
    "    for c in risk_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = (\n",
    "                df[c]\n",
    "                .replace({True: 1, False: 0, 'Yes': 1, 'No': 0, 'Y': 1, 'N': 0, 'yes': 1, 'no': 0})\n",
    "            )\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0).astype(int)\n",
    "        else:\n",
    "            df[c] = 0\n",
    "    df['risk_factor_count'] = df[risk_cols].sum(axis=1)\n",
    "\n",
    "    # Age groups & BMI categories (ensure int, no NaNs)\n",
    "    if 'age' in df.columns:\n",
    "        df['age_group'] = pd.cut(\n",
    "            df['age'],\n",
    "            bins=[0, 35, 50, 65, np.inf],\n",
    "            labels=[0, 1, 2, 3],\n",
    "            include_lowest=True,\n",
    "            right=True\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        df['age_group'] = 0\n",
    "\n",
    "    if 'bmi' in df.columns:\n",
    "        df['bmi_category'] = pd.cut(\n",
    "            df['bmi'],\n",
    "            bins=[0, 18.5, 25, 30, np.inf],\n",
    "            labels=[0, 1, 2, 3],\n",
    "            include_lowest=True,\n",
    "            right=True\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        df['bmi_category'] = 0\n",
    "\n",
    "    # Replace inf just in case\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d9587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created:\n",
      "- pulse_pressure: Systolic - Diastolic BP\n",
      "- health_score: Composite lifestyle score\n",
      "- risk_factor_count: Total number of risk factors\n",
      "- age_group: Age categorization\n",
      "- bmi_category: BMI classification\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering\n",
    "train_processed = create_features(train_processed)\n",
    "test_processed = create_features(test_processed)\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(\"- pulse_pressure: Systolic - Diastolic BP\")\n",
    "print(\"- health_score: Composite lifestyle score\")\n",
    "print(\"- risk_factor_count: Total number of risk factors\")\n",
    "print(\"- age_group: Age categorization\")\n",
    "print(\"- bmi_category: BMI classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e702b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: Preparing Data for Modeling\n",
      "============================================================\n",
      "Features shape: (7963, 30)\n",
      "Target shape: (7963,)\n",
      "Test features shape: (800, 30)\n",
      "\n",
      "Training set: (6370, 30)\n",
      "Validation set: (1593, 30)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 5: PREPARE DATA FOR MODELING\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Preparing Data for Modeling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Separate features and target\n",
    "X = train_processed.drop(['patient_id', 'heart_attack_risk'], axis=1)\n",
    "y = train_processed['heart_attack_risk']\n",
    "\n",
    "# Store test patient IDs for final submission\n",
    "test_ids = test_processed['patient_id']\n",
    "X_test = test_processed.drop('patient_id', axis=1)\n",
    "\n",
    "# Ensure both datasets have same columns\n",
    "missing_cols = set(X.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0\n",
    "\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# Split training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd374d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5.5: Final Imputation & Sanity Checks (pre-SMOTE)\n",
      "============================================================\n",
      "✓ No NaNs remain in X_train/X_val/X_test.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5.5: Final Imputation & Sanity Checks (pre-SMOTE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Ensure all features are numeric\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_val   = X_val.apply(pd.to_numeric, errors='coerce')\n",
    "X_test  = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Replace inf with NaN, then impute\n",
    "for df_ in (X_train, X_val, X_test):\n",
    "    df_.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train = pd.DataFrame(num_imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_val   = pd.DataFrame(num_imputer.transform(X_val),   columns=X_val.columns,   index=X_val.index)\n",
    "X_test  = pd.DataFrame(num_imputer.transform(X_test),  columns=X_test.columns,  index=X_test.index)\n",
    "\n",
    "# (Optional) Assert no NaNs remain\n",
    "assert not np.isnan(X_train.values).any(), \"NaNs remain in X_train after imputation\"\n",
    "assert not np.isnan(X_val.values).any(),   \"NaNs remain in X_val after imputation\"\n",
    "assert not np.isnan(X_test.values).any(),  \"NaNs remain in X_test after imputation\"\n",
    "\n",
    "print(\"✓ No NaNs remain in X_train/X_val/X_test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0714aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: Handling Class Imbalance with SMOTE\n",
      "============================================================\n",
      "Original training set: (6370, 30)\n",
      "Balanced training set: (8358, 30)\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "heart_attack_risk\n",
      "0    4179\n",
      "1    4179\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 6: HANDLE CLASS IMBALANCE\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: Handling Class Imbalance with SMOTE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "# This creates synthetic samples of the minority class\n",
    "# Important when target classes are imbalanced\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original training set: {X_train.shape}\")\n",
    "print(f\"Balanced training set: {X_train_balanced.shape}\")\n",
    "print(f\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aeca214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 7: Feature Scaling\n",
      "============================================================\n",
      "Features standardized using StandardScaler\n",
      "Mean ≈ 0, Standard Deviation ≈ 1\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 7: FEATURE SCALING\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: Feature Scaling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Standardize features (mean=0, std=1)\n",
    "# This is important for models sensitive to feature scales\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features standardized using StandardScaler\")\n",
    "print(\"Mean ≈ 0, Standard Deviation ≈ 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816ab8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m param_grid = {\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m300\u001b[39m],\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m10\u001b[39m, \u001b[32m15\u001b[39m, \u001b[32m20\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mclass_weight\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m grid_search = GridSearchCV(RandomForestClassifier(random_state=\u001b[32m42\u001b[39m), param_grid, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_balanced\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest parameters:\u001b[39m\u001b[33m\"\u001b[39m, grid_search.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:859\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    857\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    863\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    186\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    196\u001b[39m     tree._fit(\n\u001b[32m    197\u001b[39m         X,\n\u001b[32m    198\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    202\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaeefd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "  Accuracy:  0.5022\n",
      "  Precision: 0.3451\n",
      "  Recall:    0.4982\n",
      "  F1-Score:  0.4078\n",
      "  ROC-AUC:   0.5054\n",
      "\n",
      "Training Random Forest...\n",
      "  Accuracy:  0.6566\n",
      "  Precision: 0.5556\n",
      "  Recall:    0.0091\n",
      "  F1-Score:  0.0180\n",
      "  ROC-AUC:   0.4949\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Accuracy:  0.6353\n",
      "  Precision: 0.4000\n",
      "  Recall:    0.1204\n",
      "  F1-Score:  0.1851\n",
      "  ROC-AUC:   0.4970\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 8: MODEL TRAINING\n",
    "# ==========================================\n",
    "\n",
    "# We'll train multiple models and compare their performance\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=42, \n",
    "        max_iter=5000,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)  # Primary metric for this task\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")  # Most important for this task\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ac391",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# After feature selection but before scaling\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_fs = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m X_train, X_val, y_train, y_val = train_test_split(\n\u001b[32m      5\u001b[39m     X_fs, y, test_size=\u001b[32m0.2\u001b[39m, stratify=y, random_state=\u001b[32m42\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m smote = SMOTE(random_state=\u001b[32m42\u001b[39m, k_neighbors=\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\base.py:897\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:563\u001b[39m, in \u001b[36m_BaseFilter.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    561\u001b[39m     X = validate_data(\u001b[38;5;28mself\u001b[39m, X, accept_sparse=[\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params(X, y)\n\u001b[32m    568\u001b[39m score_func_ret = \u001b[38;5;28mself\u001b[39m.score_func(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 8 (Improved): Feature Selection + Advanced Models\n",
      "============================================================\n",
      "✅ Selected top features: ['age', 'chol', 'hr', 'diabetes', 'family_history', 'obesity', 'alcohol', 'prev_heart_prob', 'med_use', 'stress_lvl', 'phys_act_days', 'sleep_hr', 'systolic_bp', 'diastolic_bp', 'country_freq', 'continent_freq', 'pulse_pressure', 'risk_factor_count', 'age_group', 'bmi_category']\n",
      "\n",
      "Improved Model Performance (XGBoost):\n",
      "Accuracy:  0.6309\n",
      "Precision: 0.3876\n",
      "Recall:    0.1259\n",
      "F1-Score:  0.1901\n",
      "ROC-AUC:   0.5156\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8dbb11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.5-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.conda\\envs\\myenv\\lib\\site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\.conda\\envs\\myenv\\lib\\site-packages (from xgboost) (1.16.1)\n",
      "Downloading xgboost-3.0.5-py3-none-win_amd64.whl (56.8 MB)\n",
      "   ---------------------------------------- 0.0/56.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/56.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/56.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/56.8 MB 1.1 MB/s eta 0:00:51\n",
      "   ---------------------------------------- 0.5/56.8 MB 1.1 MB/s eta 0:00:51\n",
      "    --------------------------------------- 0.8/56.8 MB 1.2 MB/s eta 0:00:49\n",
      "    --------------------------------------- 1.0/56.8 MB 1.2 MB/s eta 0:00:48\n",
      "    --------------------------------------- 1.3/56.8 MB 1.2 MB/s eta 0:00:48\n",
      "   - -------------------------------------- 1.6/56.8 MB 1.2 MB/s eta 0:00:47\n",
      "   - -------------------------------------- 1.8/56.8 MB 1.2 MB/s eta 0:00:48\n",
      "   - -------------------------------------- 2.1/56.8 MB 1.2 MB/s eta 0:00:48\n",
      "   - -------------------------------------- 2.4/56.8 MB 1.1 MB/s eta 0:00:48\n",
      "   - -------------------------------------- 2.6/56.8 MB 1.2 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 2.9/56.8 MB 1.2 MB/s eta 0:00:47\n",
      "   -- ------------------------------------- 3.1/56.8 MB 1.2 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 3.1/56.8 MB 1.2 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 3.4/56.8 MB 1.2 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 3.7/56.8 MB 1.1 MB/s eta 0:00:47\n",
      "   -- ------------------------------------- 3.9/56.8 MB 1.1 MB/s eta 0:00:47\n",
      "   -- ------------------------------------- 3.9/56.8 MB 1.1 MB/s eta 0:00:47\n",
      "   -- ------------------------------------- 4.2/56.8 MB 1.1 MB/s eta 0:00:48\n",
      "   --- ------------------------------------ 4.5/56.8 MB 1.1 MB/s eta 0:00:48\n",
      "   --- ------------------------------------ 4.5/56.8 MB 1.1 MB/s eta 0:00:48\n",
      "   --- ------------------------------------ 4.7/56.8 MB 1.1 MB/s eta 0:00:49\n",
      "   --- ------------------------------------ 5.0/56.8 MB 1.1 MB/s eta 0:00:49\n",
      "   --- ------------------------------------ 5.0/56.8 MB 1.1 MB/s eta 0:00:49\n",
      "   --- ------------------------------------ 5.2/56.8 MB 1.1 MB/s eta 0:00:49\n",
      "   --- ------------------------------------ 5.5/56.8 MB 1.0 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 5.5/56.8 MB 1.0 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 5.8/56.8 MB 1.0 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 6.0/56.8 MB 1.0 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 6.0/56.8 MB 1.0 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 6.3/56.8 MB 1.0 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 6.6/56.8 MB 1.0 MB/s eta 0:00:51\n",
      "   ---- ----------------------------------- 6.8/56.8 MB 1.0 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 7.1/56.8 MB 1.0 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 7.1/56.8 MB 1.0 MB/s eta 0:00:50\n",
      "   ----- ---------------------------------- 7.3/56.8 MB 1.0 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 7.6/56.8 MB 1.0 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 7.9/56.8 MB 1.0 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 7.9/56.8 MB 1.0 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 8.1/56.8 MB 1.0 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 8.4/56.8 MB 1.0 MB/s eta 0:00:48\n",
      "   ------ --------------------------------- 8.7/56.8 MB 1.0 MB/s eta 0:00:48\n",
      "   ------ --------------------------------- 8.9/56.8 MB 1.0 MB/s eta 0:00:47\n",
      "   ------ --------------------------------- 9.4/56.8 MB 1.0 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 9.7/56.8 MB 1.1 MB/s eta 0:00:45\n",
      "   ------ --------------------------------- 9.7/56.8 MB 1.1 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 10.0/56.8 MB 1.0 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 10.2/56.8 MB 1.1 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 10.5/56.8 MB 1.1 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 10.7/56.8 MB 1.1 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 11.0/56.8 MB 1.1 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 11.3/56.8 MB 1.1 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 11.5/56.8 MB 1.1 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 11.8/56.8 MB 1.1 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 12.1/56.8 MB 1.1 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 12.3/56.8 MB 1.1 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 12.6/56.8 MB 1.1 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 12.8/56.8 MB 1.1 MB/s eta 0:00:41\n",
      "   --------- ------------------------------ 13.1/56.8 MB 1.1 MB/s eta 0:00:41\n",
      "   --------- ------------------------------ 13.4/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 13.6/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 13.6/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 13.9/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 14.2/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   --------- ------------------------------ 14.2/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 14.4/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 14.4/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 14.7/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 14.9/56.8 MB 1.1 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 15.2/56.8 MB 1.1 MB/s eta 0:00:39\n",
      "   ---------- ----------------------------- 15.5/56.8 MB 1.1 MB/s eta 0:00:39\n",
      "   ----------- ---------------------------- 15.7/56.8 MB 1.1 MB/s eta 0:00:39\n",
      "   ----------- ---------------------------- 16.0/56.8 MB 1.1 MB/s eta 0:00:39\n",
      "   ----------- ---------------------------- 16.0/56.8 MB 1.1 MB/s eta 0:00:39\n",
      "   ----------- ---------------------------- 16.3/56.8 MB 1.1 MB/s eta 0:00:38\n",
      "   ----------- ---------------------------- 16.5/56.8 MB 1.1 MB/s eta 0:00:38\n",
      "   ----------- ---------------------------- 16.8/56.8 MB 1.1 MB/s eta 0:00:38\n",
      "   ----------- ---------------------------- 16.8/56.8 MB 1.1 MB/s eta 0:00:38\n",
      "   ----------- ---------------------------- 16.8/56.8 MB 1.1 MB/s eta 0:00:38\n",
      "   ------------ --------------------------- 17.6/56.8 MB 1.1 MB/s eta 0:00:37\n",
      "   ------------ --------------------------- 17.8/56.8 MB 1.1 MB/s eta 0:00:37\n",
      "   ------------ --------------------------- 17.8/56.8 MB 1.1 MB/s eta 0:00:37\n",
      "   ------------ --------------------------- 18.1/56.8 MB 1.1 MB/s eta 0:00:37\n",
      "   ------------ --------------------------- 18.4/56.8 MB 1.1 MB/s eta 0:00:37\n",
      "   ------------ --------------------------- 18.4/56.8 MB 1.1 MB/s eta 0:00:37\n",
      "   ------------- -------------------------- 18.6/56.8 MB 1.1 MB/s eta 0:00:36\n",
      "   ------------- -------------------------- 18.9/56.8 MB 1.1 MB/s eta 0:00:36\n",
      "   ------------- -------------------------- 19.1/56.8 MB 1.1 MB/s eta 0:00:36\n",
      "   ------------- -------------------------- 19.4/56.8 MB 1.1 MB/s eta 0:00:36\n",
      "   ------------- -------------------------- 19.4/56.8 MB 1.1 MB/s eta 0:00:36\n",
      "   ------------- -------------------------- 19.7/56.8 MB 1.1 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 19.9/56.8 MB 1.1 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 20.2/56.8 MB 1.1 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 20.2/56.8 MB 1.1 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 20.4/56.8 MB 1.1 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 20.7/56.8 MB 1.1 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 21.0/56.8 MB 1.1 MB/s eta 0:00:34\n",
      "   -------------- ------------------------- 21.2/56.8 MB 1.1 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 21.5/56.8 MB 1.1 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 22.0/56.8 MB 1.1 MB/s eta 0:00:33\n",
      "   --------------- ------------------------ 22.3/56.8 MB 1.1 MB/s eta 0:00:32\n",
      "   --------------- ------------------------ 22.5/56.8 MB 1.1 MB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 23.1/56.8 MB 1.1 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 23.3/56.8 MB 1.1 MB/s eta 0:00:31\n",
      "   ---------------- ----------------------- 23.9/56.8 MB 1.1 MB/s eta 0:00:30\n",
      "   ---------------- ----------------------- 24.1/56.8 MB 1.1 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 24.4/56.8 MB 1.1 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 24.6/56.8 MB 1.1 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 24.9/56.8 MB 1.1 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 25.2/56.8 MB 1.1 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 25.4/56.8 MB 1.1 MB/s eta 0:00:29\n",
      "   ------------------ --------------------- 25.7/56.8 MB 1.1 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 26.0/56.8 MB 1.1 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 26.5/56.8 MB 1.1 MB/s eta 0:00:27\n",
      "   ------------------ --------------------- 26.7/56.8 MB 1.1 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 27.0/56.8 MB 1.1 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 27.3/56.8 MB 1.1 MB/s eta 0:00:26\n",
      "   ------------------- -------------------- 27.5/56.8 MB 1.1 MB/s eta 0:00:26\n",
      "   ------------------- -------------------- 28.0/56.8 MB 1.1 MB/s eta 0:00:26\n",
      "   ------------------- -------------------- 28.3/56.8 MB 1.1 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 28.6/56.8 MB 1.2 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 28.8/56.8 MB 1.2 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 29.1/56.8 MB 1.2 MB/s eta 0:00:24\n",
      "   -------------------- ------------------- 29.6/56.8 MB 1.2 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 29.9/56.8 MB 1.2 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 30.1/56.8 MB 1.2 MB/s eta 0:00:23\n",
      "   --------------------- ------------------ 30.4/56.8 MB 1.2 MB/s eta 0:00:23\n",
      "   --------------------- ------------------ 30.7/56.8 MB 1.2 MB/s eta 0:00:23\n",
      "   --------------------- ------------------ 30.9/56.8 MB 1.2 MB/s eta 0:00:23\n",
      "   --------------------- ------------------ 31.2/56.8 MB 1.2 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 31.5/56.8 MB 1.2 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 31.7/56.8 MB 1.2 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 32.2/56.8 MB 1.2 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 32.5/56.8 MB 1.2 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 32.8/56.8 MB 1.2 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 33.3/56.8 MB 1.2 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 33.8/56.8 MB 1.2 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 34.3/56.8 MB 1.2 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 34.6/56.8 MB 1.2 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 35.4/56.8 MB 1.2 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 35.9/56.8 MB 1.2 MB/s eta 0:00:17\n",
      "   ------------------------- -------------- 36.4/56.8 MB 1.3 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 37.2/56.8 MB 1.3 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 37.5/56.8 MB 1.3 MB/s eta 0:00:16\n",
      "   -------------------------- ------------- 38.0/56.8 MB 1.3 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 38.5/56.8 MB 1.3 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 39.1/56.8 MB 1.3 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 39.3/56.8 MB 1.3 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 40.1/56.8 MB 1.3 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 40.4/56.8 MB 1.3 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 40.9/56.8 MB 1.3 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 41.2/56.8 MB 1.3 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 41.7/56.8 MB 1.3 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 41.9/56.8 MB 1.3 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 42.5/56.8 MB 1.3 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 43.0/56.8 MB 1.4 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 43.5/56.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 44.0/56.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 44.3/56.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 44.8/56.8 MB 1.4 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 45.1/56.8 MB 1.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 45.6/56.8 MB 1.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 46.1/56.8 MB 1.4 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 46.4/56.8 MB 1.4 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 46.9/56.8 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 47.2/56.8 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 47.4/56.8 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 47.7/56.8 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 48.0/56.8 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 48.2/56.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 48.5/56.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 48.8/56.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 49.0/56.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 49.3/56.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 49.5/56.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 49.8/56.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 50.1/56.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 50.3/56.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 50.6/56.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 50.6/56.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 50.9/56.8 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 51.1/56.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 51.6/56.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 51.9/56.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 52.4/56.8 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 52.7/56.8 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 53.0/56.8 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 53.2/56.8 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 53.5/56.8 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 54.0/56.8 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 54.5/56.8 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 54.8/56.8 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 55.3/56.8 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  55.6/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.1/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.4/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.8/56.8 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c038e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c3517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9249f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
